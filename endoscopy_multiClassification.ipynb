{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://githubtocolab.com/Hitsh987/projet_IARN/blob/master/endoscopy_multiClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdcIR6UN_D81"
      },
      "source": [
        "# Classification of Anomalies in Gastrointestinal Tract through Endoscopic Imagery with Deep Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW3nv3tL_D8_"
      },
      "source": [
        "### we used opencv for preproccesing the image dataset\n",
        "\n",
        "### we used tensorflow and keras libarary for machine learning stuff\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "szA8_rS4_D9B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras import applications\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIFBaYT9eteW"
      },
      "source": [
        "### first we download dataset and save dataset directory name\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QL2jVzDC_FyQ",
        "outputId": "03a9f8a6-e161-4a0d-b204-3c37d37e9e5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset path: /root/.keras/datasets/kvasir-dataset\n"
          ]
        }
      ],
      "source": [
        "def download_dataset(URL, dataset_name):\n",
        "    path_to_zip = tf.keras.utils.get_file(\n",
        "        f\"{dataset_name}.zip\", origin=URL, extract=True\n",
        "    )\n",
        "    path = os.path.join(os.path.dirname(path_to_zip), dataset_name)\n",
        "    return path\n",
        "\n",
        "\n",
        "dataset_name = \"kvasir-dataset\"\n",
        "URL = f\"https://datasets.simula.no/downloads/kvasir/{dataset_name}.zip\"\n",
        "\n",
        "dataset_dir = download_dataset(URL, dataset_name)\n",
        "print(f\"Dataset path: {dataset_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pve7GXowe6QT"
      },
      "source": [
        "### define the categories the dataset categories(class)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGjH7jwdLwDG",
        "outputId": "15d12bf6-308b-412f-ace2-e0bf548fd57e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of categories:  8\n",
            "categories:\n",
            "  1.dyed-lifted-polyps\n",
            "  2.dyed-resection-margins\n",
            "  3.esophagitis\n",
            "  4.normal-cecum\n",
            "  5.normal-pylorus\n",
            "  6.normal-z-line\n",
            "  7.polyps\n",
            "  8.ulcerative-colitis\n"
          ]
        }
      ],
      "source": [
        "def get_dataCategories(dataset_dir):\n",
        "    categories = [\n",
        "        folder_name\n",
        "        for folder_name in os.listdir(dataset_dir)\n",
        "        if os.path.isdir(os.path.join(dataset_dir, folder_name))\n",
        "    ]\n",
        "\n",
        "    return sorted(categories)\n",
        "\n",
        "\n",
        "categories = get_dataCategories(dataset_dir)\n",
        "nbr_categories = len(categories)\n",
        "print(\"number of categories: \", nbr_categories)\n",
        "print(\"categories:\")\n",
        "for i, c in enumerate(categories):\n",
        "    print(f\"  {i+1}.{c}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hd0t7zpLfEaq"
      },
      "source": [
        "### using the opencv, we read images from dataset directory and resize images to 100\\*100\n",
        "\n",
        "### then insert:\n",
        "\n",
        "- the resized image in `X`\n",
        "- class lable in `y`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdfMW6BshBKL",
        "outputId": "8f70e524-537a-4455-ed55-742106df8adf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X: (4000, 100, 100, 3)\n",
            "y: (4000,)\n"
          ]
        }
      ],
      "source": [
        "# load dataset\n",
        "def create_dataset(datadir, categories, img_wid, img_high):\n",
        "    X = []\n",
        "    y = []\n",
        "    for category in categories:\n",
        "        path = os.path.join(datadir, category)\n",
        "        class_num = categories.index(category)\n",
        "        for img in os.listdir(path):\n",
        "            try:\n",
        "                img_array = cv2.imread(os.path.join(path, img))\n",
        "                X.append(cv2.resize(img_array, (img_wid, img_high)))\n",
        "                y.append(class_num)\n",
        "            except Exception as e:\n",
        "                pass\n",
        "\n",
        "    y = np.array(y)\n",
        "    X = np.array(X).reshape(y.shape[0], img_wid, img_wid, 3)\n",
        "    return X, y\n",
        "\n",
        "\n",
        "img_wid, img_high = 100, 100\n",
        "X, y = create_dataset(dataset_dir, categories, img_wid, img_high)\n",
        "\n",
        "print(f\"X: {X.shape}\")\n",
        "print(f\"y: {y.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iOxhT8vJJvR"
      },
      "outputs": [],
      "source": [
        "# split dataset to train and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, train_size=0.8, random_state=42\n",
        ")\n",
        "\n",
        "# reshape dataset to have a single channel\n",
        "width, height, channels = X_train.shape[0], X_train.shape[1], 1\n",
        "\n",
        "X_train = X_train.reshape((X_train.shape[0], width, height, channels))\n",
        "X_test = X_test.reshape((X_test.shape[0], width, height, channels))\n",
        "\n",
        "# one hot encode target values\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# confirm scale of pixels\n",
        "print('Train min=%.3f, max=%.3f' % (X_train.min(), X_train.max()))\n",
        "print('Test min=%.3f, max=%.3f' % (X_test.min(), X_test.max()))\n",
        "\n",
        "# create generator (1.0/255.0 = 0.003921568627451)\n",
        "datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
        "# prepare an iterators to scale images\n",
        "train_iterator = datagen.flow(X_train, y_train, batch_size=64)\n",
        "test_iterator = datagen.flow(X_test, y_test, batch_size=64)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation=\"relu\", input_shape=(width, height, channels)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation=\"relu\"))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation=\"relu\"))\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "# compile model\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fit model with generator\n",
        "model.fit_generator(train_iterator, steps_per_epoch=len(train_iterator), epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# evaluate model\n",
        "_, acc = model.evaluate_generator(test_iterator, steps=len(test_iterator), verbose=0)\n",
        "print('Test Accuracy: %.3f' % (acc * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WW_Mi61DUbNe",
        "outputId": "6d41c25f-684d-4ac5-b022-91407a50bbbc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pourcentage de precision: 73.85%\n"
          ]
        }
      ],
      "source": [
        "# from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# logistic_reg = LogisticRegression()\n",
        "# logistic_reg.fit(X, y)\n",
        "\n",
        "# # logistic_reg.score(X, y)\n",
        "\n",
        "# y_pred = np.zeros(X.shape[0])\n",
        "# for i in range(len(y_pred)):\n",
        "#     p = logistic_reg.predict([X[i, :]])\n",
        "#     y_pred[i] = p\n",
        "\n",
        "# prcision = np.mean(y == y_pred) * 100\n",
        "# print(\"pourcentage de precision: {:.2f}%\".format(prcision))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yP586lpOD5xe"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, train_size=0.8, random_state=42\n",
        ")\n",
        "\n",
        "X_train = X_train.astype(\"float32\")\n",
        "X_test = X_test.astype(\"float32\")\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "y_train = tf.keras.utils.to_categorical(y_train, nbr_categories)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, nbr_categories)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "3rVkbenBjao3",
        "outputId": "544ed035-1975-4cf6-95af-b39a038cd902"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-956a92eddda8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mMLP\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m         \"\"\"\n\u001b[0;32m--> 752\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_solver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, incremental)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"lbfgs\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             self._fit_lbfgs(\n\u001b[0;32m--> 441\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_units\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m             )\n\u001b[1;32m    443\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit_lbfgs\u001b[0;34m(self, X, y, activations, deltas, coef_grads, intercept_grads, layer_units)\u001b[0m\n\u001b[1;32m    545\u001b[0m                 \u001b[0;34m\"gtol\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m             },\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         )\n\u001b[1;32m    549\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_optimize_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lbfgs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m--> 610\u001b[0;31m                                 callback=callback, **options)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_loss_grad_lbfgs\u001b[0;34m(self, packed_coef_inter, X, y, activations, deltas, coef_grads, intercept_grads)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_coef_inter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         loss, coef_grads, intercept_grads = self._backprop(\n\u001b[0;32m--> 235\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept_grads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m         )\n\u001b[1;32m    237\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoef_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_backprop\u001b[0;34m(self, X, y, activations, deltas, coef_grads, intercept_grads)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             self._compute_loss_grad(\n\u001b[0;32m--> 314\u001b[0;31m                 \u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept_grads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             )\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_compute_loss_grad\u001b[0;34m(self, layer, n_samples, activations, deltas, coef_grads, intercept_grads)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mThis\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mdoes\u001b[0m \u001b[0mbackpropagation\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mspecified\u001b[0m \u001b[0mone\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \"\"\"\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mcoef_grads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0mcoef_grads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoefs_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mcoef_grads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "nn = MLPClassifier(\n",
        "    # hidden_layer_sizes=(hidden_layer_size,),\n",
        "    activation=\"logistic\",\n",
        "    solver=\"lbfgs\",\n",
        "    alpha=1,\n",
        "    max_iter=1500,\n",
        ")\n",
        "nn.fit(X, y)\n",
        "\n",
        "# precision = nn.score(X, y)\n",
        "score = nn.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", round(score[0], 2))\n",
        "print(\"Test accuracy:\", round(score[1], 2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oCd_zyxqclf"
      },
      "source": [
        "### Show random image for each category\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nJYio23jke-"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "st, end = 0, 500\n",
        "for i in range(8):\n",
        "    plt.subplot(2, 4, i + 1)\n",
        "    idx = np.random.randint(st, end)\n",
        "    st = end + 1\n",
        "    end = (i + 2) * 500\n",
        "    # plt.imshow(X[idx][:,:,::-1])\n",
        "    plt.imshow(X[idx], cmp=\"gray\")\n",
        "    plt.title(f\"{i+1}. {categories[y[idx]]}\")\n",
        "    plt.axis(\"off\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXNa645-_D9L"
      },
      "source": [
        "### partition the data into training and testing splits using 80% of the data for training and the remaining 20% for testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwRt7gP0_D9M"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, train_size=0.8, random_state=42\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXeI0qUq_D9M"
      },
      "source": [
        "### then preprocess dataset by scaling all pixel intensities to the range [0, 1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9Di3vli_D9N"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.astype(\"float32\")\n",
        "X_test = X_test.astype(\"float32\")\n",
        "X_train /= 255\n",
        "X_test /= 255\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJipKTf8_D9N"
      },
      "source": [
        "### encode the labels (which are currently strings) as integers and then one-hot encode them\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSrqDSPe_D9O"
      },
      "outputs": [],
      "source": [
        "y_train = tf.keras.utils.to_categorical(y_train, nbr_categories)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, nbr_categories)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXHRvA5S_D9O"
      },
      "source": [
        "### import VGG19 model (for Transfer learning)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgAJN700_D9P"
      },
      "outputs": [],
      "source": [
        "model = applications.VGG19(\n",
        "    weights=\"imagenet\", include_top=False, input_shape=(img_wid, img_high, 3)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBRdDAmy_D9P"
      },
      "source": [
        "### first we iterate through the model and make 20 layer non-trainable\n",
        "\n",
        "### then add pooling layer and add some dense layers with relu activation\n",
        "\n",
        "### we used categorical_crossentropy loss function and Adam as the optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNZsezd8_D9Q"
      },
      "outputs": [],
      "source": [
        "# for layer in model.layers[:20]:\n",
        "for layer in model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "x = model.output\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = tf.keras.layers.Dense(1024, activation=\"relu\")(x)\n",
        "x = tf.keras.layers.Dropout(0.5)(x)\n",
        "x = tf.keras.layers.Dense(512, activation=\"relu\")(x)\n",
        "x = tf.keras.layers.Dropout(0.5)(x)\n",
        "x = tf.keras.layers.Dense(256, activation=\"relu\")(x)\n",
        "x = tf.keras.layers.Dropout(0.5)(x)\n",
        "x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
        "predictions = tf.keras.layers.Dense(8, activation=\"softmax\")(x)\n",
        "model_final = tf.keras.models.Model(model.input, predictions)\n",
        "model_final.compile(\n",
        "    loss=\"categorical_crossentropy\", optimizer=\"Adam\", metrics=[\"accuracy\"]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQ6oicf2_D9R"
      },
      "source": [
        "### then we fit the dataset to train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x32--JWA_D9R",
        "outputId": "3822ac39-8833-4a86-e863-772222de8b69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/65\n",
            "13/13 [==============================] - 963s 75s/step - loss: 2.1030 - accuracy: 0.1538 - val_loss: 1.9542 - val_accuracy: 0.4125\n",
            "Epoch 2/65\n",
            "13/13 [==============================] - 955s 75s/step - loss: 1.8459 - accuracy: 0.2641 - val_loss: 1.4272 - val_accuracy: 0.4588\n",
            "Epoch 3/65\n",
            "13/13 [==============================] - 956s 75s/step - loss: 1.4498 - accuracy: 0.3688 - val_loss: 1.1275 - val_accuracy: 0.5387\n",
            "Epoch 4/65\n",
            "13/13 [==============================] - 956s 75s/step - loss: 1.2149 - accuracy: 0.4628 - val_loss: 0.9698 - val_accuracy: 0.5612\n",
            "Epoch 5/65\n",
            "13/13 [==============================] - 954s 74s/step - loss: 1.0771 - accuracy: 0.5244 - val_loss: 0.8967 - val_accuracy: 0.5537\n",
            "Epoch 6/65\n",
            "13/13 [==============================] - 956s 75s/step - loss: 0.9711 - accuracy: 0.5659 - val_loss: 0.8833 - val_accuracy: 0.5688\n",
            "Epoch 7/65\n",
            "13/13 [==============================] - 956s 75s/step - loss: 0.9036 - accuracy: 0.5913 - val_loss: 0.8089 - val_accuracy: 0.6175\n",
            "Epoch 8/65\n",
            "13/13 [==============================] - 958s 75s/step - loss: 0.8546 - accuracy: 0.6216 - val_loss: 0.7148 - val_accuracy: 0.6675\n",
            "Epoch 9/65\n",
            "13/13 [==============================] - 963s 75s/step - loss: 0.8263 - accuracy: 0.6319 - val_loss: 0.7299 - val_accuracy: 0.6463\n",
            "Epoch 10/65\n",
            "13/13 [==============================] - 959s 75s/step - loss: 0.7641 - accuracy: 0.6722 - val_loss: 0.6862 - val_accuracy: 0.6538\n",
            "Epoch 11/65\n",
            "13/13 [==============================] - 962s 75s/step - loss: 0.7250 - accuracy: 0.6906 - val_loss: 0.6512 - val_accuracy: 0.7113\n",
            "Epoch 12/65\n",
            "13/13 [==============================] - 963s 75s/step - loss: 0.7101 - accuracy: 0.6950 - val_loss: 0.6487 - val_accuracy: 0.7188\n",
            "Epoch 13/65\n",
            "13/13 [==============================] - 961s 75s/step - loss: 0.7379 - accuracy: 0.6787 - val_loss: 0.7549 - val_accuracy: 0.6500\n",
            "Epoch 14/65\n",
            "13/13 [==============================] - 962s 75s/step - loss: 0.7011 - accuracy: 0.7050 - val_loss: 0.6428 - val_accuracy: 0.7050\n",
            "Epoch 15/65\n"
          ]
        }
      ],
      "source": [
        "history = model_final.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    # batch_size=32,\n",
        "    batch_size=256,\n",
        "    # epochs=8,\n",
        "    epochs=65,\n",
        "    verbose=1,\n",
        "    validation_data=(X_test, y_test),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TF3KoAa_D9S"
      },
      "outputs": [],
      "source": [
        "score = model_final.evaluate(X_test, y_test, verbose=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-TP0UuJ_D9S"
      },
      "outputs": [],
      "source": [
        "print(\"Test loss:\", round(score[0], 2))\n",
        "print(\"Test accuracy:\", round(score[1], 2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62QPKFcZ_D9T"
      },
      "outputs": [],
      "source": [
        "print(history.history.keys())\n",
        "\n",
        "plt.plot(history.history[\"accuracy\"])\n",
        "plt.plot(history.history[\"val_accuracy\"])\n",
        "\n",
        "plt.title(\"model accuracy\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history[\"loss\"])\n",
        "plt.plot(history.history[\"val_loss\"])\n",
        "plt.title(\"model loss\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend([\"train\", \"test\"], loc=\"upper right\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYdrvmjkVC9Z"
      },
      "outputs": [],
      "source": [
        "X_train.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMIkdZN4WP9f"
      },
      "outputs": [],
      "source": [
        "a = X_train[500]\n",
        "a.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6a7naEMVD_2"
      },
      "outputs": [],
      "source": [
        "plt.imshow(X_train[500][:, :, ::-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4gsdigNiUM8"
      },
      "outputs": [],
      "source": [
        "a = X_train[500]\n",
        "a.ndim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkfSXeiWmjzK"
      },
      "outputs": [],
      "source": [
        "b = np.argmax(y_train[500])\n",
        "b = categories[b]\n",
        "b\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKuj_cccVeV8"
      },
      "outputs": [],
      "source": [
        "a = X_train[500]\n",
        "a = a[None, :, :, :]\n",
        "print(a.shape)\n",
        "predict_x = model_final.predict(a)\n",
        "classes_x = np.argmax(predict_x, axis=1)\n",
        "print(categories[classes_x[0]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfyAzXDDiFsp"
      },
      "outputs": [],
      "source": [
        "def predict_categorie_img(img, model, categories):\n",
        "    try:\n",
        "        img = img[None, :, :, :]\n",
        "    except:\n",
        "        raise TypeError(\"test image dimension != 3\")\n",
        "    predict = model.predict(img)\n",
        "    classes = np.argmax(predict, axis=1)\n",
        "    return categories[classes[0]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQjN5I0kbTIm"
      },
      "outputs": [],
      "source": [
        "def cvtRGB(img):\n",
        "    return cv2.cvtColor(img.copy(), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, imgs in enumerate(images):\n",
        "    plt.subplot(3, 5, i + 1)\n",
        "    idx = np.random.randint(len(imgs))\n",
        "    plt.imshow(cvtRGB(imgs[idx]))\n",
        "    plt.grid(\"off\")\n",
        "    plt.title(categories[i] + \" \" + str(idx))\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "transfer_learning_VGG19.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
